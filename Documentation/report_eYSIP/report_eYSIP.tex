\documentclass[a4paper,12pt,oneside]{book}

%-------------------------------Start of the Preable------------------------------------------------
\usepackage[english]{babel}
\usepackage{blindtext}
%packagr for hyperlinks
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\urlstyle{same}
%use of package fancy header
\usepackage{fancyhdr}
\setlength\headheight{26pt}
\fancyhf{}
%\rhead{\includegraphics[width=1cm]{logo}}
\lhead{\rightmark}
\rhead{\includegraphics[width=1cm]{logo.png}}
\fancyfoot[RE, RO]{\thepage}
\fancyfoot[CE, CO]{\href{http://www.e-yantra.org}{www.e-yantra.org}}

\pagestyle{fancy}

%use of package for section title formatting
\usepackage{titlesec}
\titleformat{\chapter}
  {\Large\bfseries} % format
  {}                % label
  {0pt}             % sep
  {\huge}           % before-code
 
%use of package tcolorbox for colorful textbox
\usepackage[most]{tcolorbox}
\tcbset{colback=cyan!5!white,colframe=cyan!75!black,halign title = flush center}

\newtcolorbox{mybox}[1]{colback=cyan!5!white,
colframe=cyan!75!black,fonttitle=\bfseries,
title=\textbf{\Large{#1}}}

%use of package marginnote for notes in margin
\usepackage{marginnote}

%use of packgage watermark for pages
%\usepackage{draftwatermark}
%\SetWatermarkText{\includegraphics{logo}}
\usepackage[scale=2,opacity=0.1,angle=0]{background}
\backgroundsetup{
contents={\includegraphics{logo}}
}

%use of newcommand for keywords color
\usepackage{xcolor}
\newcommand{\keyword}[1]{\textcolor{red}{\textbf{#1}}}

%package for inserting pictures
\usepackage{graphicx}

%package for highlighting
\usepackage{color,soul}

%new command for table
\newcommand{\head}[1]{\textnormal{\textbf{#1}}}


%----------------------End of the Preamble---------------------------------------


\begin{document}

%---------------------Title Page------------------------------------------------
\begin{titlepage}
\raggedright
{\Large e-YSIP 2017\\[1cm]}
{\Huge\scshape Vegetable Identification Using Transfer Learning \\[.1in]}
\vfill
\begin{flushright}
{\large Sanket Shanbhag \\}
{\large Supriya Suresh \\}
{\large Saurav, Naveen and Khalid \\}
{\large Duration of Internship: $ 22/05/2017-07/07/2017 $ \\}
\end{flushright}

{\itshape 2017, e-Yantra Publication}
\end{titlepage}
%-------------------------------------------------------------------------------

\chapter[Project Tag]{Vegetable Identification Using \\ Transfer Learning}
\section*{Abstract}
This project aims to create a system for automatically logging farm produce from the green house. An image of the produce is captured and this image is fed to a neural network based on the Inception-v3 model provided by \href{https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html}{Google}. This system then identifies the vegetable and classifies it into different classes(vegetables) it has been trained on. Also, a re-training system is implemented which automatically trains any new images captured and thereby further increasing the accuracy of the system over time.

\subsection*{Completion status}
The system has been successfully trained for 18 classes (vegetables) achieving accuracy of 99.5\% on a separate test set. New vegetables can also be added easily. Also a system has been implemented for auto-training any new images captured on the system every 4 days.

\section{Hardware parts}
This project focuses on Machine Learning to identify vegetables, and hardware design is not in the scope of this project. To view the hardware specifications and requirements please refer to \href{https://github.com/eYSIP-2017/eYSIP-2017_Development_of_Web_Interface_for_GH_Farm_Produce}{this} logging project and \href{https://github.com/Ankurrpanwar26/eYSIP-2016-Farm-Produce-Logging-and-Monitoring}{this} project which describes the smart weighing machine.


\newpage
\section{Software used}
\begin{itemize}
  \item \href{https://www.python.org/downloads/release/python-352/}{Python 3.5.2}
  \item \href{https://pypi.python.org/pypi/tensorflow/1.2.0}{Tensorflow 1.2.0}
  \item \href{https://github.com/google/prettytensor}{PrettyTensor 0.7.4} 
  \item Installation Steps:
  \begin{itemize}
  	\item Open a terminal in the project base folder and type: \\
  	\texttt{pip3 install -r requirements.txt}
  \end{itemize}
\end{itemize}


\section{Software and Code}
The complete code is available \href{https://github.com/eYSIP-2017/eYSIP-2017_Vegetable-Identification-Using-Transfer-Learning}{here}. It is divided into various folders each with a self contained module of the project.

\subsection{Downloading and formatting data}
The folder \texttt{download\_data} contains python scripts and shell scripts for downloading and formatting data from a list of URLS or from \href{http://www.image-net.org/}{ImageNet}. Information on using these scripts can be found in the \href{https://github.com/eYSIP-2017/eYSIP-2017_Vegetable-Identification-Using-Transfer-Learning/wiki/Downloading-and-formatting-data.}{wiki page}.

\subsection{Steps for adding new crop to system}
Go to download\textunderscoredata/setupdata.py and modify the wnetids global dictionary to include the names of the vegetable and corresponding wnet-id as key-value pairs. This will create a folder for each vegetable with a corresponding text file containing links to all the images.

\subsection{Transfer Learning}
The code for transfer learning can be found in the \texttt{transfer\_on\_inception\_v3} folder. The \texttt{transferveg.py} file contains the code for running the model.
We removed the final layer of the Inception-v3 model and add 3 fully connected layers of 4096, 2048, and 1024 nodes with a dropout layer after the 2048-node layer.
This helped us achieve an accuracy of 99.5\%

\subsection{Module Integration}
The \texttt{ghfarm.py} script contained in the \texttt{module\_integration/raspberrypi} folder will run on the raspberry pi on the weighing machine. It interfaces with the server to predict the crop and sends data once the image is taken.

\subsection{Auto-Training}
The \texttt{AutoTrain} folder contains code for running a server to accept images and scripts for adding cron tasks to run the autotraining code every 4 days.

\section{Use and Demo}
Final Setup Image
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\linewidth]{setup.jpg}
	\caption{Setup}
	\label{fig:setup}
\end{figure}


Instructions for using the weighing machine can be found \href{https://github.com/eYSIP-2017/eYSIP-2017_Vegetable-Identification-Using-Transfer-Learning/tree/master/Documentation/Manuals}{here}.\\
For Retraining and training from scratch, please refer to the \href{https://github.com/eYSIP-2017/eYSIP-2017_Vegetable-Identification-Using-Transfer-Learning/wiki}{github wiki}.

 

\section{Future Work}
This project focuses mainly on limited number of vegetables mentioned in the \href{https://github.com/eYSIP-2017/eYSIP-2017_Vegetable-Identification-Using-Transfer-Learning/wiki/List-of-Vegetables}{wiki}. In the future it can be extended to include other vegetables and fruits.

\section{Challenges}
\begin{enumerate}
	\item Lack of proper training data.
	\item Every vegetable has different varieties so our second challenge was to find enough training data to encompass all the varieties.
\end{enumerate}

\section{Results}

\begin{itemize}
	\item The system works well while identifying vegetables with a distinct texture and color such as tomato, bittergourd, ladyfinger etc. However, while identifying similar-looking vegetables like fenugreek, basil and coriander, we found that the system tends to make mistakes and/or have low confidence.
	\item Vegetables were tested on the following test cases:
	\subitem 1. Variation of light - Dark, less light, normal light, bright day light(like window open).
	\subitem 2. Variation in Vegetable storage device - zip-lock bag, tray, basket, direct on base-plate of machine.
	\subitem 3. Variation in ambient environment captured by camera.\\\\\\	
	  It was observed that the system predicted correct results with pretty good accuracy even when exposed to the above mentioned conditions.
	  \newpage
	  \item \Large\textbf{Results for some Test cases were as follows:}
	  
	  \begin{figure}[!ht]
	  	\centering
	  	\includegraphics[width=0.5\linewidth]{brinjal-dark}
	  	\caption{Brinjal under dark ligt condition} \textbf{Obtained Confidence: 92.08\%}
	  	\label{fig:brinjal-dark}
	  \end{figure}
	 \hfill
	  \begin{figure}[!ht]
	  	\centering
	  	\includegraphics[width=0.5\linewidth]{"lady_finger - light"}
	  	\caption{Lady finger under bright light condition}\textbf{ Obtained Confidence: 95.93\%}
	  	\label{fig:ladyfinger---light}
	  \end{figure}
  \hfill
	  \begin{figure}[!ht]
	  	\centering
	  	\includegraphics[width=0.5\linewidth]{cucumber-tray}
	  	\caption{Cucumber on tray}\textbf{ Obtained Confidence: 99.5\%}
	  	\label{fig:cucumber-tray}
	  \end{figure}
  \hfill
	  \begin{figure}[!ht]
	  	\centering
	  	\includegraphics[width=0.5\linewidth]{"tomato - ziplock"}
	  	\caption{Tomato inside zip-lock}\textbf{ Obtained Confidence: 92.03\%}
	  	\label{fig:tomato---ziplock}
	  \end{figure}
  %\hfill
	  \begin{figure}[!ht]
	  	\centering
	  	\includegraphics[width=0.5\linewidth]{"onion - hand"}
	  	\caption{Onion in ambient conditon}\textbf{ Obtained Confidence: 93.00\%}
	  	\label{fig:onion---hand}
	  \end{figure}
	  
\end{itemize}

\newpage
\section{References}
\begin{enumerate}
	\item \href{https://arxiv.org/abs/1512.00567}{Inception v3 model} by Google.
	\item \href{http://cs231n.github.io/convolutional-networks/}{Architecture of Convolutional Neural Networks} by Stanford.
\end{enumerate}

\end{document}

