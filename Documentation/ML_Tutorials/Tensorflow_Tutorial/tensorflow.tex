%
\documentclass[Proceedings]{ascelike}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{red},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\NameTag{}
%
%
\begin{document}
%
% You will need to make the title all-caps
\title{TensorFlow Tutorial}
%
\author{
Sanket Shanbhag \\
e-YSIP 2017
}
%
\maketitle
%
\section{Introduction}
%\begin{abstract}
This document describes some basic concepts on how to use \href{https://www.tensorflow.org/install/}{TensorFlow}. Before using this document, you should first read the document on basic machine learning concepts. Also, some familiarity with neural networks and how they function is assumed. If you are new to neural networks, you should also read through \href{http://neuralnetworksanddeeplearning.com/chap1.html}{this} introductory tutorial.
Although TensorFlow is available for Python 2.7, for this tutorial we will be using Python 3 along with TensorFlow version 1.2.0.

%\end{abstract}
TensorFlow is a machine learning library which provides both high level and low level API's for creating and running models. This allows you to create complex models faster than other libraries but also provides fine-grained control if required. It is specifically designed for neural networks; however, other machine learning algorithms are also available. Models in TensorFlow are defined using data-flow graphs. 

Many scientific and numerical libraries in Python optimize mathematical operations by using another lower level language to perform those operations. TensorFlow builds up on this by computing the entire data-flow graph in a lower level language thereby avoiding the cost of context switching between two languages.

\section{How to use this tutorial}
It is not enough just to read through this document. Try to write each line and run it yourself. After you are done, try out the things mentioned at the end. If you are stuck at any point, the best place to find more information about the TensorFlow API is the \href{https://www.tensorflow.org/api_docs/}{API docs}.

\section{Setting up}
Before we use Tensorflow, we must import it:

\begin{lstlisting}[language=Python]
import tensorflow as tf
\end{lstlisting}

The central unit of data in TensorFlow is the tensor. A tensor can simply be thought of as a multidimensional array. A tensor's rank is its number of dimensions. Here are some examples of tensors:

\begin{lstlisting}[language=Python]
3 # a rank 0 tensor; this is a scalar with shape []
[1, 2, 3] # a rank 1 tensor; this is a vector with shape [3]
[[1, 2, 3], [4, 5, 6]] # a rank 2 tensor; a matrix with shape [2, 3]
[[[1, 2, 3]], [[7, 8, 9]]] # a rank 3 tensor with shape [2, 1, 3]
\end{lstlisting}

To create a computational graph, we create nodes and then run a session on these nodes to generate the output. Each node takes zero or more tensors as inputs and produces a tensor as an output.

\section{Inputs and Sessions}

\subsection{Constants}
One type of node is a constant. Like all TensorFlow constants, it takes no inputs, and it outputs a value it stores internally.
\begin{lstlisting}[language=Python]
# Use dtype to optionally specify a type
node = tf.constant(42.0, dtype=tf.float32)
\end{lstlisting}

The various data-types in tensorflow which can be used are found \href{https://www.tensorflow.org/programmers_guide/dims_types}{here}.

At this stage node is a tensor object that, when evaluated will hold the value 42.0. To actually evaluate this node, we have to run the computational graph in a session. We use the \texttt{run()} function of the Session() object for this job. The \texttt{run()} function will run the graph and returns the output of the object that is passed to it. We can also pass in multiple objects to get a tuple of results.

\begin{lstlisting}[language=Python]
# Start a new Tensorflow session
sess = tf.Session()
print(sess.run([node])) # Prints 42.0
\end{lstlisting}

\subsection{Placeholders}
Constants are not that interesting, as they cannot be changed. To accept external inputs at run-time, we use placeholders.

\begin{lstlisting}[language=Python]
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = tf.add(a,b)
\end{lstlisting}

To give a value to the placeholder in the computational graph, use the \texttt{feed\_dict} argument in the \texttt{run} function of the Session object to pass a python dictionary specifying the placeholders with their values as key-value pairs.

\begin{lstlisting}[language=Python]
# Prints 42.0!
print(sess.run(adder_node, feed_dict={a:18.0, b:24.0}))
\end{lstlisting}

To assign placeholders of a higher rank, use the \texttt{shape} argument to specify the shape of the tensor. If any of the dimensions can be arbitary, for example when using arbitary number of training samples, you can use \texttt{None} instead.

\begin{lstlisting}[language=Python]
# Takes a tensor of dimensions [None, 500]
a = tf.placeholder(tf.float32, shape=[None, 500])
# Tensor of dimensions [40, 50, 100]
b = tf.placeholder(tf.float32, shape=[40, 50, 100])
\end{lstlisting}

\subsection{Variables}
Variables allow us to add trainable parameters to a graph. A variable maintains state in the graph across calls to \texttt{run()}. You add a variable to the graph by constructing an instance of the class Variable. They are constructed with a type and initial value:

\begin{lstlisting}[language=Python]
# Create some variables.
W = tf.Variable([.3], dtype=tf.float32)
# tf.random_normal(): Outputs random values from a normal 
# distribution which will be of shape [784, 200]. 
# See the docs for more info
weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),
                      name="weights")
# tf.zeros(): Creates a tensor with all elements set to zero 
# with shape [200]
biases = tf.Variable(tf.zeros([200]), name="biases")
\end{lstlisting}

Like \texttt{tf.random\_normal()} and \texttt{tf.zeros()} shown above, TensorFlow provides a collection of ops that produce tensors often used for \href{https://www.tensorflow.org/versions/r1.0/api_guides/python/constant_op}{initialization from constants or random values}.

Variables are not initialized when you call \texttt{tf.Variable}. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:

\begin{lstlisting}
sess.run(tf.global_variables_initializer())
\end{lstlisting}

We have now gone over all the components required to define a working model. Now we will look at how to use these to create a model and run it.

\section{Simple Linear Model}

In this section, we will create a simple model for classification on the easily available \href{https://en.wikipedia.org/wiki/MNIST_database}{MNIST} data-set and train it. Some familiarity with neural networks, activation functions and backpropagation are prerequisites for this section.

\subsection{Downloading and Formatting Data}
We will be using a single hidden layer of 500 nodes and an output layer of 10 classes.

\begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("./data/", one_hot = True)
\end{lstlisting}

This will download the MNIST data-set into a \texttt{data} folder in the current working directory. The data-set has been loaded as so-called One-Hot encoding. This means the labels have been converted from a single number to a vector whose length equals the number of possible classes. All elements of the vector are zero except for the \texttt{i}'th element which is one and means the class is \texttt{i}.
For example, if the class value is 4, then it's one-hot encoded vector will be:

\begin{lstlisting}[language=Python]
# 4th value is 1, everything else is 0
[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
\end{lstlisting}

If you would like to use your own data-set, check out \href{https://agray3.github.io/2016/11/29/Demystifying-Data-Input-to-TensorFlow-for-Deep-Learning.html}{this} tutorial.

\subsection{Setting up Hyperparamaters}
\begin{lstlisting}[language=Python]
# Hyperparameters
# 10 digits to identify
n_classes = 10
# Train 100 images at a time to avoid using up too much RAM
batch_size = 100
# We will be using a single hidden layer of 500 neurons
n_nodes_hl1 = 500
# The images are 28x28 pixels each
img_size_flat = 28 * 28
\end{lstlisting}

\subsection{Setting up the model}
To input images to our model, we will first flatten them into a single dimensional vector of length \texttt{img\_size\_flat}, and then feed this vector into our model. To do this, we will define a few placeholders:

\begin{lstlisting}[language=Python]
# Input placeholder for flattened images
X = tf.placeholder('float', shape=[None, img_size_flat])
# Input vector for true class labels.
y = tf.placeholder('float')
\end{lstlisting}

Now we define the weights and biases for the hidden layer and output layer:

\begin{lstlisting}[language=Python]
# Hidden layer
hidden_layer_weights = tf.Variable(tf.random_normal([img_size_flat, 
                                   n_nodes_hl1]))
hidden_layer_biases = tf.Variable(tf.random_normal([n_nodes_hl1]))
# Output layer
output_layer_weights = tf.Variable(tf.random_normal([n_nodes_hl1,
                                   n_classes]))
output_layer_biases = tf.Variable(tf.random_normal([n_classes]))
\end{lstlisting}

\subsection{Setting up a saver object}
Running large models can take a significant amount of time. If your program crashes in the middle of execution, you may lose all your trained data on the model. To save your trained model in case of a crash, we can use tensorflow Saver objects as shown:

\begin{lstlisting}[language=Python]
import os
# Declare a saver object
saver = tf.train.Saver()
# Directory to save checkpoints to
save_dir = 'checkpoints/'
# Create the directory if it doesn't exist
if not os.path.exists(save_dir):
    os.makedirs(save_dir)
# Prefix to attach with which checkpoints will be saved
save_path = os.path.join(save_dir, 'saved')
\end{lstlisting}

A single Saver object can only save one instance of your model. To save multiple copies during different iterations, you will have to create multiple Saver objects.

To restore the saved checkpoints, after declaring bejeyour model, simply call the following function:

\begin{lstlisting}[language=Python]
# Restore saved model from save_path
saver.restore(sess=session,save_path=save_path)
\end{lstlisting}

This will allow you to use a saved model.


\subsection{Connecting the Layers}
We now define the relationship between our layers by matrix multiplying the data with the weights and adding the biases. We use the \href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}{ReLu} activation function. Other activation functions are also available in tensorflow and can be found \href{https://www.tensorflow.org/api_guides/python/nn}{here}.

\begin{lstlisting}[language=Python]
# tf.matmul will perform matrix-multiplication on its inputs.
# tf.add will add its arguments together.
l1 = tf.add(tf.matmul(X,hidden_layer_weights), 
                      hidden_layer_biases)
# Using ReLu activation
l1 = tf.nn.relu(l1)
# You can use the + operator instead of using the tf.add() function
output = tf.matmul(l1,output_layer_weights) + output_layer_biases
\end{lstlisting}

\subsection{Training}
We now define a function to train this network. We will use the \href{https://en.wikipedia.org/wiki/Gradient_descent}{GradientDescent} Optimizer to reduce the \href{https://en.wikipedia.org/wiki/Mean_squared_error}{mean squared error (MSE)} loss function. Other optimizers in tensorflow can be found \href{https://www.tensorflow.org/api_guides/python/train#Optimizers}{here}.

\begin{lstlisting}[language=Python]
# Takes the model as input and runs the session on it.
def train_neural_network(x):
    prediction = x
    # Softmax function
    smx = tf.nn.softmax_cross_entropy_with_logits(logits=prediction,
                                                  labels=y)
    # Define the cost function
    cost = tf.reduce_mean(smx)
    # Use the GradientDescentOptimizer with a learning rate of 0.5 
    # to minimize the cost
    optimizer = tf.train.GradientDescentOptimizer(0.5).\
                minimize(cost)
    # Number of epochs to train for
    hm_epochs = 20
    
    # Start a new tensorflow session
    with tf.Session() as sess:
        # Initialize the global variables
        sess.run(tf.global_variables_initializer())
        # Run a loop for the total number of epochs
        for epoch in range(hm_epochs):
            epoch_loss = 0
            # Divide the data set into batches of size batch_size
            batchquot = int(mnist.train.num_examples/batch_size)
            
            for _ in range(batchquot):
                # Get a batch of images and labels
                xt, yt = mnist.train.next_batch(batch_size)
                
                # Run the optimizer to minimize the 
                # cost on the batch
                _, c = sess.run([optimizer, cost], 
                                feed_dict={X:xt, y:yt})
                # Add the cost to our epoch loss
                epoch_loss += c
                
                # Save this epoch so we can continue
                # in case of crash
                saver.save(sess=session,
                           save_path=save_path)

            print('Epoch', epoch+1, 'completed out of',hm_epochs,
                                  'loss:',epoch_loss)
        
        # Calculate and print accuracy
        correct = tf.equal(tf.argmax(prediction, 1), 
                           tf.argmax(y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:',accuracy.eval({X:mnist.test.images, 
                                         y:mnist.test.labels}))
        
        
\end{lstlisting}

To begin training, we simply pass our model to the function:

\begin{lstlisting}[language=Python]
train_neural_network(output)
\end{lstlisting}

\subsection{Result}
Running the above network gives us the following results:

\begin{lstlisting}
Epoch 1 completed out of 20 loss: 4112.72234179
Epoch 2 completed out of 20 loss: 292.313294291
Epoch 3 completed out of 20 loss: 179.839554987
Epoch 4 completed out of 20 loss: 126.560542699
Epoch 5 completed out of 20 loss: 97.231446553
Epoch 6 completed out of 20 loss: 76.5352744549
Epoch 7 completed out of 20 loss: 64.625726237
Epoch 8 completed out of 20 loss: 54.0737959952
Epoch 9 completed out of 20 loss: 47.0280316649
Epoch 10 completed out of 20 loss: 41.1886193645
Epoch 11 completed out of 20 loss: 36.654014512
Epoch 12 completed out of 20 loss: 32.950176964
Epoch 13 completed out of 20 loss: 29.3913420243
Epoch 14 completed out of 20 loss: 26.9070334777
Epoch 15 completed out of 20 loss: 24.5256814114
Epoch 16 completed out of 20 loss: 21.9067392419
Epoch 17 completed out of 20 loss: 20.2392465728
Epoch 18 completed out of 20 loss: 18.6161083714
Epoch 19 completed out of 20 loss: 17.0158358993
Epoch 20 completed out of 20 loss: 15.9119512606
Accuracy: 0.943
\end{lstlisting}
\newpage

\section{Things to Try}
After you are done with the above model, you can try out the following things and see how they impact speed, accuracy and performance:
\begin{itemize}
    \item Increase the number of epochs.
    \item Vary the learning rate.
    \item Change the optimizer used.
    \item Add 2 more layers.
\end{itemize}


\end{document}

